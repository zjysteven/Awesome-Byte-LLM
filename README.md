<p align='center'>
  <img src='assets/teaser.webp' alt='A teaser figure generated by DALL-E' width=70%>
</p>

# Awesome Byte-Based Large Language Models

## Introduction

Recently, there has been a growing interest in studying byte-based large language models (LLMs). These models eliminate the need for tokenization and operate directly on raw bytesâ€”the universal format of the digital world. Byte-based models offer several promising advantages, including:

- **Enhanced robustness and generalization:** By removing the heuristic biases introduced by tokenization, these models could achieve better adaptability.
- **Cross-modality scalability:** Since all data can be represented as bytes, these models naturally extend to multiple modalities.

This repository serves as an ongoing collection of papers and resources focused on byte-based LLMs.

## Papers

The meaning of most fields are clear by their names. "Date" is the time that the work is released/made public (e.g., the timestamp of its first arXiv version). "Summary" is a one-sentence summary of the paper.

| Title | Date (MM/YY) | Source | Code | Summary |
|-------|:------------:|:------:|:----:|---------|
| [Byte Latent Transformer: Patches Scale Better Than Tokens](https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/) | 12/24 | Meta | [official](https://github.com/facebookresearch/blt) | <details>The paper introduces the Byte Latent Transformer (BLT), a novel byte-level large language model (LLM) that dynamically groups bytes into patches, improving inference efficiency, robustness, and scalability beyond tokenization-based models.</details> |
| [MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/pdf/2305.07185) | 05/23 | NeurIPS'23 | [unofficial](https://github.com/lucidrains/MEGABYTE-pytorch) | <details>The paper introduces MEGABYTE, a multiscale Transformer architecture that segments sequences into patches, enabling efficient modeling of million-byte sequences with sub-quadratic self-attention, enhanced feedforward computation, and improved decoding parallelism, achieving competitive performance on tasks like long-context language modeling, image generation, and audio modeling.</details> |


## Contributing

Contributions are always welcome. There are two ways to add a new paper:
1. The easiest way is to open an issue, where I have a template for you to fill out.
2. If you would like to be listed as a contributor, you add the paper to `papers.json` and make a pull request. Please do NOT directly edit `README.md`.
