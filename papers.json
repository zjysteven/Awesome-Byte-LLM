[
  {
    "title": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers",
    "date": "2023-05",
    "link": "https://arxiv.org/pdf/2305.07185",
    "conference": "NeurIPS'23",
    "summary": "The paper introduces MEGABYTE, a multiscale Transformer architecture that segments sequences into patches, enabling efficient modeling of million-byte sequences with sub-quadratic self-attention, enhanced feedforward computation, and improved decoding parallelism, achieving competitive performance on tasks like long-context language modeling, image generation, and audio modeling."
  }
]
