[
    {
        "title": "CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation",
        "date": "03/21",
        "link": "https://arxiv.org/pdf/2103.06874",
        "source": "TACL",
        "summary": "The paper introduces CANINE, a tokenization-free neural encoder that directly processes character sequences using a downsampling-transformer-upsampling architecture, achieving competitive multilingual performance on tasks like TYDI QA and NER while offering efficiency and robustness advantages over traditional subword-based models.",
        "code": "https://github.com/google-research/language/tree/master/language/canine",
        "code_official": true
    },
    {
        "title": "ByT5: Towards a token-free future with pre-trained byte-to-byte models",
        "date": "05/21",
        "link": "https://arxiv.org/pdf/2105.13626",
        "source": "TACL",
        "summary": "The paper presents ByT5, a token-free byte-to-byte variant of the T5 Transformer that processes raw UTF-8 byte sequences without tokenization, achieving competitive performance across multilingual NLP tasks, offering robustness to noise, and demonstrating improved efficiency, especially in low-resource or multilingual contexts.",
        "code": "https://github.com/google-research/byt5",
        "code_official": true
    },
    {
        "title": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers",
        "date": "05/23",
        "link": "https://arxiv.org/pdf/2305.07185",
        "source": "NeurIPS'23",
        "summary": "The paper introduces MEGABYTE, a multiscale Transformer architecture that segments sequences into patches, enabling efficient modeling of million-byte sequences with sub-quadratic self-attention, enhanced feedforward computation, and improved decoding parallelism, achieving competitive performance on tasks like long-context language modeling, image generation, and audio modeling.",
        "code": "https://github.com/lucidrains/MEGABYTE-pytorch",
        "code_official": false
    },
    {
        "title": "Bytes Are All You Need: Transformers Operating Directly On File Bytes",
        "date": "06/23",
        "link": "https://arxiv.org/pdf/2306.00238",
        "source": "TMLR",
        "summary": "The paper introduces ByteFormer, a modality-independent transformer architecture that operates directly on file bytes, eliminating the need for modality-specific processing, and demonstrates superior performance on classification tasks across images, audio, and mixed-modality data.",
        "code": "https://github.com/apple/corenet/tree/main/projects/byteformer",
        "code_official": true
    },
    {
        "title": "MambaByte: Token-free Selective State Space Model",
        "date": "01/24",
        "link": "https://arxiv.org/pdf/2401.13660",
        "source": "COLM'24",
        "summary": "MambaByte introduces a token-free selective state space model (SSM) that enables efficient language modeling of byte-level sequences with a fixed-sized memory state, outperforming subword and byte-level Transformers on language modeling tasks while offering improved robustness, efficiency, and speculative decoding for faster inference.",
        "code": "https://github.com/jxiw/MambaByte",
        "code_official": true
    },
    {
        "title": "Beyond Language Models: Byte Models are Digital World Simulators",
        "date": "02/24",
        "link": "https://arxiv.org/abs/2402.19155",
        "source": "arXiv",
        "summary": "The paper introduces bGPT, a next-byte prediction model that operates directly on binary data to simulate diverse digital world processes, achieving state-of-the-art performance in modalities like text, audio, images, symbolic music conversion, and even CPU behavior simulation with over 99.99% accuracy.",
        "code": "https://github.com/sanderwood/bgpt",
        "code_official": true
    },
    {
        "title": "Byte Latent Transformer: Patches Scale Better Than Tokens",
        "date": "12/24",
        "link": "https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/",
        "source": "Meta",
        "summary": "The paper introduces the Byte Latent Transformer (BLT), a novel byte-level large language model (LLM) that dynamically groups bytes into patches, improving inference efficiency, robustness, and scalability beyond tokenization-based models.",
        "code": "https://github.com/facebookresearch/blt",
        "code_official": true
    }
]
