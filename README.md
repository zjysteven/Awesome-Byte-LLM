# Awesome Byte-Based Large Language Models

## Introduction

Recently, there has been a growing interest in studying byte-based large language models (LLMs). These models eliminate the need for tokenization and operate directly on raw bytesâ€”the universal format of the digital world. Byte-based models offer several promising advantages, including:

- **Enhanced robustness and generalization:** By removing the heuristic biases introduced by tokenization, these models could achieve better adaptability.
- **Cross-modality scalability:** Since all data can be represented as bytes, these models naturally extend to multiple modalities.


This repository serves as an ongoing collection of papers and resources focused on byte-based LLMs.


## Paper list

The meaning of most fields are clear by their names. "Date" is the time that the work is released/made public (e.g., the timestamp of its first arXiv version). "AI Summary" is a one-sentence summary of the paper from Scholar GPT. "Remarks" are something that I personally feel like worth mentioning.

| Title | Date | Conference | Code | AI Summary | Remarks |
|-------|---------|------|------|---------|--------|
| [MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/abs/1234.5678) | Alice, Yu | 2024 | [Link](https://arxiv.org/abs/1234.5678) | A survey on byte-based large language models. |
| [Efficient Byte-Based Transformers](https://arxiv.org/abs/8765.4321) | Charlie, Dana | 2023 | [Link](https://arxiv.org/abs/8765.4321) | Exploration of efficient methods for byte-level transformers. |


## Contributing

Contributions are always welcomed. There are two ways of adding new papers.
1. The easiest way would be to open an issue, where I have set up a template.
2. If you would like to have your contribution recorded by git or be listed as a contributor, then please add new entries in `papers.json` and open a pull request. Do not directly modify the README.
